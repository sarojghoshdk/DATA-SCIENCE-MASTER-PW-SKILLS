{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "822cfa53-617b-4399-94dc-827db46ef14a",
   "metadata": {},
   "source": [
    "## Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245b6ac-a37d-4a81-9cc2-e7408eaacb80",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "**Simple linear regression** is a statistical method that analyzes the relationship between two continuous variables by fitting a linear equation to the data. It estimates the relationship between the dependent variable (y) and the independent variable (x) by finding the best-fit line that minimizes the sum of squared errors between the observed data points and the predicted values.\n",
    "\n",
    ">For example, let's say you want to predict a person's weight (y) based on their height (x). You collect data on 20 individuals and plot the data on a scatter plot. You can then use simple linear regression to find the equation of the line that best fits the data and can be used to make predictions about an individual's weight based on their height.\n",
    "\n",
    "### Equation for simple linear regression:\n",
    "$\\hat{y} = \\beta_0 + \\beta_1 x$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\hat{y}$ is the predicted value of the dependent variable\n",
    "* x is the value of the independent variable for which we want to make a prediction\n",
    "* $\\beta_0$ is the intercept (the value of y when x=0)\n",
    "* $\\beta_1$ is the slope (the change in y for a unit change in x)\n",
    "\n",
    "**Multiple linear regression**, on the other hand, is a statistical method that analyzes the relationship between multiple independent variables (x1, x2, x3, etc.) and a single dependent variable (y). It estimates the effect of each independent variable on the dependent variable while controlling for the effects of the other independent variables.\n",
    "\n",
    ">For example, let's say you want to predict a person's salary (y) based on their education level (x1), years of experience (x2), and gender (x3). You collect data on 100 individuals and use multiple linear regression to find the equation that best fits the data and can be used to predict a person's salary based on their education level, years of experience, and gender.\n",
    "\n",
    "### Equation for Multiple Linear Regression :\n",
    "\n",
    "$\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\hat{y}$ is the predicted value of the dependent variable\n",
    "* $x_1$, $x_2$, ..., $x_k$ are the values of the independent variables for which we want to make a prediction\n",
    "* $\\beta_0$ is the intercept (the value of y when all independent variables are 0)\n",
    "* $\\beta_1$, $\\beta_2$, ..., $\\beta_k$ are the slopes (the change in y for a unit change in each independent variable) for each independent variable $x_1$, $x_2$, ..., $x_k$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10a9836-3d3a-4425-a696-cbf0df7277b7",
   "metadata": {},
   "source": [
    "## Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcd78d1-6647-499b-8961-809359011f74",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "The assumptions of linear regression are:\n",
    "\n",
    "- **Linearity:** The relationship between the dependent variable and each independent variable is linear. This means that the relationship between the dependent variable and each independent variable should be approximately straight-line.\n",
    "\n",
    "- **Independence:** The observations are independent of each other. This means that the value of the dependent variable for one observation should not be related to the value of the dependent variable for another observation.\n",
    "\n",
    "- **Normality:** The errors are normally distributed. This means that the distribution of the residuals should be approximately normal.\n",
    "\n",
    "- **No Multicollinearity:** There is no perfect multicollinearity between the independent variables. This means that the independent variables should not be highly correlated with each other.\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, various diagnostic tests can be performed. These tests include:\n",
    "\n",
    "- **Residual plots:** plotting the residuals against the predicted values and independent variables to detect patterns that violate the assumptions of linearity, homoscedasticity, and normality.\n",
    "\n",
    "- **Normal probability plots:** plotting the residuals against the expected normal distribution to check for normality.\n",
    "\n",
    "- **Outlier detection:** identifying and examining the influence of outliers on the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b9f47c-cf79-4598-adf4-9481affe7719",
   "metadata": {},
   "source": [
    "![Assumptions of Linear Regression](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/B97-Header-Image.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da85b00e-1d86-4997-9476-24a424909f6b",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d194017e-f4a1-485a-a2e1-39f481dd361c",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "In a linear regression model, the slope and intercept represent the relationship between the dependent variable and independent variable(s). Specifically:\n",
    "\n",
    "**The intercept** (denoted by \"b0\") represents the predicted value of the dependent variable when all independent variables are equal to zero. It is the point where the regression line intersects with the y-axis.\n",
    "\n",
    "**The slope** (denoted by \"b1\") represents the change in the dependent variable for each one-unit increase in the independent variable. It is the steepness of the regression line and reflects the strength and direction of the relationship between the dependent variable and the independent variable.\n",
    "\n",
    "Here is an example using a real-world scenario:\n",
    "\n",
    "Suppose we want to understand the relationship between a person's height (independent variable) and their weight (dependent variable). We collect data on 50 individuals and use a simple linear regression model to analyze the data. The regression model provides the following output:\n",
    "\n",
    ">Intercept (b0) = 50<br>\n",
    "Slope (b1) = 2\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "The intercept (50) represents the predicted weight of a person with a height of zero, which is not meaningful in this context. Therefore, we do not interpret the intercept in this example.\n",
    "\n",
    "The slope (2) represents the predicted increase in weight for each one-unit increase in height. So, for every one-inch increase in height, we predict an increase of two pounds in weight. This means that the relationship between height and weight is positive and strong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6a64e2-3df2-4edb-abdf-856996744c72",
   "metadata": {},
   "source": [
    "## Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e879313-23c2-4e23-a899-a36ea1cf0b30",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "**Gradient descent** is a popular optimization algorithm used in machine learning to minimize the cost function or the objective function. The goal of gradient descent is to find the optimal values of the parameters (also called weights) that minimize the cost function and make accurate predictions on new data.\n",
    "\n",
    "Gradient descent is used in many machine learning algorithms, such as linear regression, logistic regression, neural networks, and support vector machines. By minimizing the cost function using gradient descent, we can find the optimal values of the parameters that maximize the accuracy of the model on the training data and generalize well to new data.\n",
    "\n",
    "### Equation for Gradient Descent:\n",
    "$\\theta_j = \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}$\n",
    "\n",
    "where $\\theta$ is a vector of the parameters of the model, $\\alpha$ is the learning rate, $J(\\theta)$ is the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1dd892-aaab-4d47-a17d-65c7ca6ad035",
   "metadata": {},
   "source": [
    "![Gradient Descent](https://editor.analyticsvidhya.com/uploads/631731_P7z2BKhd0R-9uyn9ThDasA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbc3524-945f-497f-903d-857645eab9fd",
   "metadata": {},
   "source": [
    "### Partial Derrivatives calculation of gradient descent in Linear Regression\n",
    "\n",
    "1. Loss Function:\n",
    "\n",
    "$J(m,c) =\\frac{1}{n}\\displaystyle\\sum_{i=1} ^ {n} (y_i-(mx_i+c))^2$\n",
    "\n",
    "2. Partial Derrivative wrt intercept:\n",
    "\n",
    "$\\frac{\\partial J(m,c)}{\\partial c} = \\frac{-2}{n}\\displaystyle\\sum_{i=1} ^ {n} (y_i-(mx_i+c))$\n",
    "\n",
    "3. Partial Derrivative wrt slope :\n",
    "\n",
    "$\\frac{\\partial J(m,c)}{\\partial m} = \\frac{-2}{n}\\displaystyle\\sum_{i=1} ^ {n} (y_i-(mx_i+c))*x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bd3a46-2bc3-48f5-b518-3053aa9a76e8",
   "metadata": {},
   "source": [
    "## Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfd0d47-7240-48bc-bc0b-d4dbc744e658",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "In a multiple linear regression model, we predict the value of a dependent variable (also called the response variable) based on two or more independent variables (also called predictors or features). The multiple linear regression model is an extension of the simple linear regression model, which predicts the value of the dependent variable based on a single independent variable.\n",
    "\n",
    "The multiple linear regression model differs from the simple linear regression model in several ways.\n",
    "\n",
    "- Firstly, the multiple linear regression model involves multiple independent variables instead of just one. This means that the model can capture more complex relationships between the dependent and independent variables, allowing us to control for the effects of other variables on the dependent variable.\n",
    "\n",
    "- Secondly, the multiple linear regression model requires more parameters to estimate (i.e., b1, b2, ..., bp), which makes the model more complex and computationally intensive.\n",
    "\n",
    "### The mathematical formula for multiple linear regression can be represented as:\n",
    "\n",
    "$\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\hat{y}$ is the predicted value of the dependent variable\n",
    "* $x_1$, $x_2$, ..., $x_k$ are the values of the independent variables for which we want to make a prediction\n",
    "* $\\beta_0$ is the intercept (the value of y when all independent variables are 0)\n",
    "* $\\beta_1$, $\\beta_2$, ..., $\\beta_k$ are the slopes (the change in y for a unit change in each independent variable) for each independent variable $x_1$, $x_2$, ..., $x_k$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da19ffbb-5ee7-425e-a859-f356585aafe8",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f605ed96-42d3-415c-b2f6-f4fad99e9ddf",
   "metadata": {},
   "source": [
    "## Ans: \n",
    "\n",
    "Multicollinearity in multiple linear regression refers to a situation where there is a high correlation between two or more independent variables in the model. This high correlation makes it difficult to estimate the effect of each independent variable on the dependent variable accurately. In other words, the independent variables are no longer independent, and it is challenging to distinguish their individual effects on the dependent variable.\n",
    "\n",
    "To detect multicollinearity, one can calculate the correlation matrix between the independent variables in the dataset. If the correlation coefficients are high (close to 1 or -1), there may be multicollinearity. Additionally, one can use the variance inflation factor (VIF) to identify the extent of multicollinearity. A VIF value of 1 indicates no multicollinearity, while a value greater than 1 suggests the presence of multicollinearity.\n",
    "\n",
    "To address multicollinearity, one can use various techniques. One of the most common techniques is to remove one of the highly correlated independent variables from the model. Alternatively, one can combine the correlated independent variables into a single variable, or use regularization techniques such as ridge regression or Lasso regression, which can reduce the impact of multicollinearity.\n",
    "\n",
    "**Detecting multicollinearity can be done using several methods, such as:**\n",
    "\n",
    "- **Correlation matrix:** A correlation matrix can be used to identify the correlation between the independent variables. If two or more variables have a high correlation coefficient (close to 1 or -1), it suggests that there may be multicollinearity in the model.\n",
    "\n",
    "- **Variance Inflation Factor (VIF):** The VIF is a measure of the extent to which the variance of the estimated regression coefficients is inflated due to multicollinearity. A VIF value greater than 5 or 10 suggests that there may be multicollinearity in the model.\n",
    "\n",
    "- **Eigenvalues:** The eigenvalues of the correlation matrix can be used to detect multicollinearity. If one or more eigenvalues are close to zero, it suggests that there may be multicollinearity in the model.\n",
    "\n",
    "**Some of the ways we can address this issue are:**\n",
    "\n",
    "- **Drop one or more of the highly correlated variables:** One way to address multicollinearity is to drop one or more of the highly correlated variables from the model. This can help to reduce the correlation between the remaining variables and improve the accuracy of the model's predictions.\n",
    "\n",
    "- **Combine the highly correlated variables:** Another way to address multicollinearity is to combine the highly correlated variables into a single variable. For example, if two variables measure the same concept but in different ways, we can create a composite variable that captures both concepts.\n",
    "\n",
    "- **Regularization:** Regularization techniques, such as Ridge Regression and Lasso Regression, can be used to address multicollinearity by penalizing the magnitude of the regression coefficients. This can help to reduce the impact of the highly correlated variables and improve the accuracy of the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a36092-7eba-435b-9b03-54d65bf8477f",
   "metadata": {},
   "source": [
    "## Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7403c2bf-4ed7-4818-8f15-221a0290de65",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "**A polynomial regression model** is a type of regression analysis that allows for non-linear relationships between the dependent variable and one or more independent variables. Unlike linear regression, which assumes a linear relationship between the dependent variable and the independent variables, polynomial regression can model more complex relationships.\n",
    "\n",
    "The main difference between linear regression and polynomial regression is the nature of the relationship between the dependent variable and the independent variable. In linear regression, the relationship is assumed to be linear, while in polynomial regression, the relationship can be non-linear and can take on a more complex shape. Polynomial regression allows for more flexibility in modeling the relationship between the variables and can provide a better fit to the data when the relationship is non-linear.\n",
    "\n",
    "### Formula for Polynomial regression is :\n",
    "\n",
    "$\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_n x^n$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\hat{y}$ is the predicted value of the dependent variable\n",
    "* $x$ is the values of the independent variables for which we want to make a prediction\n",
    "* $\\beta_0$ is the intercept (the value of y when all independent variables are 0)\n",
    "* $\\beta_1$, $\\beta_2$, ..., $\\beta_n$ are the slopes (the change in y for a unit change in each independent variable) for each polynomial degree $x$, $x^2$, ..., $x^n$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b205b4be-a77a-469c-b4ef-af5dd187af63",
   "metadata": {},
   "source": [
    "![Linear vs Polynomial](https://magnimetrics.com/wp-content/uploads/2021/02/polynomial-regression-01.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e9d3e-7926-4077-904a-ff8b40e02c0a",
   "metadata": {},
   "source": [
    "## Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42512320-bbb8-4780-a679-92f44be8157b",
   "metadata": {},
   "source": [
    "## Ans:\n",
    "\n",
    "Polynomial regression is a type of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial. The polynomial regression model can provide a better fit to the data than a linear model when the relationship between x and y is nonlinear.\n",
    "\n",
    "**Advantages of polynomial regression:**\n",
    "\n",
    "- **Flexibility:** Polynomial regression can capture non-linear relationships between the variables, whereas linear regression assumes a linear relationship.\n",
    "\n",
    "- **Higher accuracy:** Polynomial regression can provide a better fit to the data when the relationship between the variables is non-linear, leading to higher accuracy of predictions compared to linear regression.\n",
    "\n",
    "- **Better fit: In cases where a linear model does not fit the data well, polynomial regression can provide a better fit.\n",
    "\n",
    "**Disadvantages of polynomial regression:**\n",
    "\n",
    "- **Overfitting:** Polynomial regression models with high-degree polynomials can overfit the data, leading to poor performance on new data.\n",
    "\n",
    "- **Complexity:** Higher degree polynomial regression models are more complex and difficult to interpret.\n",
    "\n",
    ">In situations where the relationship between the variables is nonlinear, and a linear regression model does not fit the data well, polynomial regression can be a useful alternative. However, it is important to be cautious when using high-degree polynomial models to avoid overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a890e4f1-9802-4d28-9d9d-a3b9635fe736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
